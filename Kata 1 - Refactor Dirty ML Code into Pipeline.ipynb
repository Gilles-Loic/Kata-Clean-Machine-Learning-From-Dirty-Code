{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kata 1: Refactor Dirty ML Code into Pipeline\n",
    "\n",
    "Let's convert dirty machine learning code into clean code using a [Pipeline](https://stackoverflow.com/a/60303302/2476920) - which is the [Pipe and Filter Design Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/pipes-and-filters) for Machine Learning. \n",
    "\n",
    "At first you may still wonder *why* using this Design Patterns is good. You'll realize just how good it is in the 2nd [Clean Machine Learning Kata](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code) when you'll do AutoML. Pipelines will give you the ability to easily manage the hyperparameters and the hyperparameter space, on a per-step basis. You'll also have the good code structure for training, saving, reloading, and deploying using any library you want without hitting a wall when it'll come to serializing your whole trained pipeline for deploying in prod.\n",
    "\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "It'll be downloaded automatically for you in the code below. \n",
    "\n",
    "We're using a Human Activity Recognition (HAR) dataset captured using smartphones. The [dataset](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) can be found on the UCI Machine Learning Repository. \n",
    "\n",
    "### The task\n",
    "\n",
    "Classify the type of movement amongst six categories from the phones' sensor data:\n",
    "- WALKING,\n",
    "- WALKING_UPSTAIRS,\n",
    "- WALKING_DOWNSTAIRS,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING.\n",
    "\n",
    "### Video dataset overview\n",
    "\n",
    "Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"Video of the experiment\" width=\"400\" height=\"300\" border=\"10\" /></a>\n",
    "  <a href=\"https://youtu.be/XOEN9W05_4A\"><center>[Watch video]</center></a>\n",
    "</p>\n",
    "\n",
    "### Details about the input data\n",
    "\n",
    "The dataset's description goes like this:\n",
    "\n",
    "> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. \n",
    "\n",
    "Reference: \n",
    "> Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n",
    "\n",
    "That said, I will use the almost raw data: only the gravity effect has been filtered out of the accelerometer  as a preprocessing step for another 3D feature as an input to help learning. If you'd ever want to extract the gravity by yourself, you could use the following [Butterworth Low-Pass Filter (LPF)](https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform) and edit it to have the right cutoff frequency of 0.3 Hz which is a good frequency for activity recognition from body sensors.\n",
    "\n",
    "Here is how the 3D data cube looks like. So we'll have a train and a test data cube, and might create validation data cubes as well: \n",
    "\n",
    "![](time-series-data.jpg)\n",
    "\n",
    "So we have 3D data of shape `[batch_size, time_steps, features]`. If this and the above is still unclear to you, you may want to [learn more on the 3D shape of time series data](https://www.quora.com/What-do-samples-features-time-steps-mean-in-LSTM/answer/Guillaume-Chevalier-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dynamic .py file download needed: not in a Colab.\n",
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code\n",
      " cache\t\t\t\t\t\t        __pycache__\n",
      " data\t\t\t\t\t\t        README.md\n",
      " data_loading.py\t\t\t\t        requirements.txt\n",
      "'Kata 1 - Refactor Dirty ML Code into Pipeline.ipynb'   time-series-data.jpg\n",
      "'Kata 2 - AutoML Loop and HyperparameterSpace.ipynb'    time-series-data.xcf\n",
      " kata2.py\t\t\t\t\t        venv\n",
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code/data\n",
      " download_dataset.py   source.txt\t 'UCI HAR Dataset.zip'\n",
      " __MACOSX\t      'UCI HAR Dataset'\n",
      "\n",
      "Downloading...\n",
      "Dataset already downloaded. Did not download twice.\n",
      "\n",
      "Extracting...\n",
      "Dataset already extracted. Did not extract twice.\n",
      "\n",
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code/data\n",
      " download_dataset.py   source.txt\t 'UCI HAR Dataset.zip'\n",
      " __MACOSX\t      'UCI HAR Dataset'\n",
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code\n",
      " cache\t\t\t\t\t\t        __pycache__\n",
      " data\t\t\t\t\t\t        README.md\n",
      " data_loading.py\t\t\t\t        requirements.txt\n",
      "'Kata 1 - Refactor Dirty ML Code into Pipeline.ipynb'   time-series-data.jpg\n",
      "'Kata 2 - AutoML Loop and HyperparameterSpace.ipynb'    time-series-data.xcf\n",
      " kata2.py\t\t\t\t\t        venv\n",
      "\n",
      "Dataset is now located at: data/UCI HAR Dataset/\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "\n",
    "def download_import(filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        # Downloading like that is needed because of Colab operating from a Google Drive folder that is only \"shared with you\".\n",
    "        url = 'https://raw.githubusercontent.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code/master/{}'.format(filename)\n",
    "        f.write(urllib.request.urlopen(url).read())\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    download_import(\"data_loading.py\")\n",
    "    !mkdir data;\n",
    "    download_import(\"data/download_dataset.py\")\n",
    "    print(\"Downloaded .py files: dataset loaders.\")\n",
    "except:\n",
    "    print(\"No dynamic .py file download needed: not in a Colab.\")\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "!pwd && ls\n",
    "os.chdir(DATA_PATH)\n",
    "!pwd && ls\n",
    "!python download_dataset.py\n",
    "!pwd && ls\n",
    "os.chdir(\"..\")\n",
    "!pwd && ls\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install neuraxle if needed:\n",
    "try:\n",
    "    import neuraxle\n",
    "    assert neuraxle.__version__ == '0.3.4'\n",
    "except:\n",
    "    !pip install neuraxle==0.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(2947, 128, 9) (2947, 1) 0.09913992 0.39567086\n",
      "Dataset loaded!\n"
     ]
    }
   ],
   "source": [
    "# Finally load dataset!\n",
    "from data_loading import load_all_data\n",
    "X_train, y_train, X_test, y_test = load_all_data()\n",
    "print(\"Dataset loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now define and execute our ugly code: \n",
    "\n",
    "You don't need to change the functions here just below. We'll rather code this again after in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def get_fft_peak_infos(real_fft, time_bins_axis=-2):\n",
    "    \"\"\"\n",
    "    Extract the indices of the bins with maximal amplitude, and the corresponding amplitude values.\n",
    "\n",
    "    :param fft: real magnitudes of an fft. It could be of shape [N, bins, features].\n",
    "    :param time_bins_axis: axis of the frequency bins (e.g.: time axis before fft).\n",
    "    :return: Two arrays without bins. One is an int, the other is a float. Shape: ([N, features], [N, features])\n",
    "    \"\"\"\n",
    "    peak_bin = np.argmax(real_fft, axis=time_bins_axis)\n",
    "    peak_bin_val = np.max(real_fft, axis=time_bins_axis)\n",
    "    return peak_bin, peak_bin_val\n",
    "\n",
    "\n",
    "def fft_magnitudes(data_inputs, time_axis=-2):\n",
    "    \"\"\"\n",
    "    Apply a Fast Fourier Transform operation to analyze frequencies, and return real magnitudes.\n",
    "    The bins past the half (past the nyquist frequency) are discarded, which result in shorter time series.\n",
    "\n",
    "    :param data_inputs: ND array of dimension at least 1. For instance, this could be of shape [N, time_axis, features]\n",
    "    :param time_axis: axis along which the time series evolve\n",
    "    :return: real magnitudes of the data_inputs. For instance, this could be of shape [N, (time_axis / 2) + 1, features]\n",
    "             so here, we have `bins = (time_axis / 2) + 1`.\n",
    "    \"\"\"\n",
    "    fft = np.fft.rfft(data_inputs, axis=time_axis)\n",
    "    real_fft = np.absolute(fft)\n",
    "    return real_fft\n",
    "\n",
    "\n",
    "def get_fft_features(x_data):\n",
    "    \"\"\"\n",
    "    Will featurize data with an FFT.\n",
    "\n",
    "    :param x_data: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "    :return: featurized time series with FFT of shape [batch_size, features]\n",
    "    \"\"\"\n",
    "    real_fft = fft_magnitudes(x_data)\n",
    "    flattened_fft = real_fft.reshape(real_fft.shape[0], -1)\n",
    "    peak_bin, peak_bin_val = get_fft_peak_infos(real_fft)\n",
    "    return flattened_fft, peak_bin, peak_bin_val\n",
    "\n",
    "\n",
    "def featurize_data(x_data):\n",
    "    \"\"\"\n",
    "    Will convert 3D time series of shape [batch_size, time_steps, sensors] to shape [batch_size, features]\n",
    "    to prepare data for machine learning.\n",
    "\n",
    "    :param x_data: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "    :return: featurized time series of shape [batch_size, features]\n",
    "    \"\"\"\n",
    "    print(\"Input shape before feature union:\", x_data.shape)\n",
    "\n",
    "    flattened_fft, peak_bin, peak_bin_val = get_fft_features(x_data)\n",
    "    mean = np.mean(x_data, axis=-2)\n",
    "    median = np.median(x_data, axis=-2)\n",
    "    min = np.min(x_data, axis=-2)\n",
    "    max = np.max(x_data, axis=-2)\n",
    "\n",
    "    featurized_data = np.concatenate([\n",
    "        flattened_fft,\n",
    "        peak_bin,\n",
    "        peak_bin_val,\n",
    "        mean,\n",
    "        median,\n",
    "        min,\n",
    "        max,\n",
    "    ], axis=-1)\n",
    "\n",
    "    print(\"Shape after feature union, before classification:\", featurized_data.shape)\n",
    "    return featurized_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the ugly code to do ugly machine learning with it.\n",
    "\n",
    "Fit: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before feature union: (7352, 128, 9)\n",
      "Shape after feature union, before classification: (7352, 639)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Shape: [batch_size, time_steps, sensor_features]\n",
    "X_train_featurized = featurize_data(X_train)\n",
    "# Shape: [batch_size, remade_features]\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train_featurized, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before feature union: (2947, 128, 9)\n",
      "Shape after feature union, before classification: (2947, 639)\n",
      "Shape at output after classification: (2947,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Shape: [batch_size, time_steps, sensor_features]\n",
    "X_test_featurized = featurize_data(X_test)\n",
    "# Shape: [batch_size, remade_features]\n",
    "\n",
    "y_pred = classifier.predict(X_test_featurized)\n",
    "print(\"Shape at output after classification:\", y_pred.shape)\n",
    "# Shape: [batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ugly pipeline code: 0.8615541228367831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "print(\"Accuracy of ugly pipeline code:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Define Pipeline Steps and a Pipeline\n",
    "\n",
    "The kata is to fill the classes below and to use them properly in the pipeline thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.base import BaseStep, NonFittableMixin\n",
    "from neuraxle.steps.numpy import NumpyConcatenateInnerFeatures, NumpyShapePrinter, NumpyFlattenDatum\n",
    "\n",
    "class NumpyFFT(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Featurize time series data with FFT.\n",
    "\n",
    "        :param data_inputs: time series data of 3D shape: [batch_size, time_steps, sensors_readings]\n",
    "        :return: featurized data is of 2D shape: [batch_size, n_features]\n",
    "        \"\"\"\n",
    "        transformed_data = np.fft.rfft(data_inputs, axis=-2)\n",
    "        return transformed_data\n",
    "\n",
    "\n",
    "class FFTPeakBinWithValue(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will compute peak fft bins (int), and their magnitudes' value (float), to concatenate them.\n",
    "\n",
    "        :param data_inputs: real magnitudes of an fft. It could be of shape [batch_size, bins, features].\n",
    "        :return: Two arrays without bins concatenated on feature axis. Shape: [batch_size, 2 * features]\n",
    "        \"\"\"\n",
    "        time_bins_axis = -2\n",
    "        peak_bin = np.argmax(data_inputs, axis=time_bins_axis)\n",
    "        peak_bin_val = np.max(data_inputs, axis=time_bins_axis)\n",
    "        \n",
    "        # Notice that here another FeatureUnion could have been used with a joiner:\n",
    "        transformed = np.concatenate([peak_bin, peak_bin_val], axis=-1)\n",
    "        \n",
    "        return transformed\n",
    "\n",
    "\n",
    "class NumpyAbs(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a max.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.abs(data_inputs)\n",
    "\n",
    "\n",
    "class NumpyMean(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a mean.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.mean(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMedian(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a median.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.median(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMin(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a min.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.min(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMax(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a max.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.max(data_inputs, axis=-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the Pipeline with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code/venv/lib/python3.6/site-packages/neuraxle/base.py:2450: UserWarning: Named pipeline tuples must be unique. Will rename 'TrainOnlyWrapper' because it already exists.\n",
      "  \"Will rename '{}' because it already exists.\".format(class_name))\n"
     ]
    }
   ],
   "source": [
    "from neuraxle.base import Identity\n",
    "from neuraxle.pipeline import Pipeline\n",
    "from neuraxle.steps.flow import TrainOnlyWrapper\n",
    "from neuraxle.union import FeatureUnion\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # ToNumpy(),  # Cast type in case it was a list.\n",
    "    # For debugging, do this print at train-time only:\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Input shape before feature union\")),\n",
    "    # Shape: [batch_size, time_steps, sensor_features]\n",
    "    FeatureUnion([\n",
    "        # TODO in kata 1: Fill the classes in this FeatureUnion here and make them work.\n",
    "        #      Note that you may comment out some of those feature classes\n",
    "        #      temporarily and reactivate them one by one.\n",
    "        Pipeline([\n",
    "            NumpyFFT(),\n",
    "            NumpyAbs(),\n",
    "            FeatureUnion([\n",
    "                NumpyFlattenDatum(),  # Reshape from 3D to flat 2D: flattening data except on batch size\n",
    "                FFTPeakBinWithValue()  # Extract 2D features from the 3D FFT bins\n",
    "            ], joiner=NumpyConcatenateInnerFeatures())\n",
    "        ]),\n",
    "        NumpyMean(),\n",
    "        NumpyMedian(),\n",
    "        NumpyMin(),\n",
    "        NumpyMax()\n",
    "    ], joiner=NumpyConcatenateInnerFeatures()),  # The joiner will here join like this: np.concatenate([...], axis=-1)\n",
    "    # TODO in kata 2, optional: Add some feature selection right here for the motivated ones:\n",
    "    #      https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "    # TODO in kata 2, optional: Add normalization right here (if using other classifiers)\n",
    "    #      https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Shape after feature union, before classification\")),\n",
    "    # Shape: [batch_size, remade_features]\n",
    "    DecisionTreeClassifier(),\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Shape at output after classification\")),\n",
    "    # Shape: [batch_size]\n",
    "    Identity()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Code: Make the Tests Pass\n",
    "\n",
    "The 3rd test is the real deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test '_test_is_pipeline(pipeline)' succeed!\n",
      "==> Test '_test_has_all_data_preprocessors(pipeline)' succeed!\n",
      "NumpyShapePrinter: (7352, 128, 9) Input shape before feature union\n",
      "NumpyShapePrinter: (7352, 639) Shape after feature union, before classification\n",
      "NumpyShapePrinter: (7352,) Shape at output after classification\n",
      "Test accuracy score: 0.8622327790973872\n",
      "==> Test '_test_pipeline_words_and_has_ok_score(pipeline)' succeed!\n"
     ]
    }
   ],
   "source": [
    "def _test_is_pipeline(pipeline):\n",
    "    assert isinstance(pipeline, Pipeline)\n",
    "\n",
    "\n",
    "def _test_has_all_data_preprocessors(pipeline):\n",
    "    assert \"DecisionTreeClassifier\" in pipeline\n",
    "    assert \"FeatureUnion\" in pipeline\n",
    "    assert \"Pipeline\" in pipeline[\"FeatureUnion\"]\n",
    "    assert \"NumpyMean\" in pipeline[\"FeatureUnion\"]\n",
    "    assert \"NumpyMedian\" in pipeline[\"FeatureUnion\"]\n",
    "    assert \"NumpyMin\" in pipeline[\"FeatureUnion\"]\n",
    "    assert \"NumpyMax\" in pipeline[\"FeatureUnion\"]\n",
    "\n",
    "\n",
    "def _test_pipeline_words_and_has_ok_score(pipeline):\n",
    "    pipeline = pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test accuracy score:\", accuracy)\n",
    "    assert accuracy > 0.7\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tests = [_test_is_pipeline, _test_has_all_data_preprocessors, _test_pipeline_words_and_has_ok_score]\n",
    "    for t in tests:\n",
    "        try:\n",
    "            t(pipeline)\n",
    "            print(\"==> Test '{}(pipeline)' succeed!\".format(t.__name__))\n",
    "        except Exception as e:\n",
    "            print(\"==> Test '{}(pipeline)' failed:\".format(t.__name__))\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good job!\n",
    "\n",
    "Your code should now be clean after making the tests pass.\n",
    "\n",
    "## You're ready for [part 2](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code).\n",
    "\n",
    "You should now be ready for the 2nd [Clean Machine Learning Kata](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code). Note that the solutions are available in the repository above as well. \n",
    "\n",
    "## Recommended additional readings and learning resources: \n",
    "\n",
    "- For more info on clean machine learning, you may want to read [How to Code Neat Machine Learning Pipelines](https://www.neuraxio.com/en/blog/neuraxle/2019/10/26/neat-machine-learning-pipelines.html).\n",
    "- For reaching higher performances, you could use a [LSTM Recurrent Neural Network](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) and refactoring it into a neat pipeline as you've created here, now by [using TensorFlow in your ML pipeline](https://github.com/Neuraxio/Neuraxle-TensorFlow).\n",
    "- You may as well want to request [more training and coaching for your ML or time series processing projects](https://www.neuraxio.com/en/time-series-solution) from us if you need.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "git_kata_ml",
   "language": "python",
   "name": "git_kata_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
