{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kata 2: Refactor Dirty ML Code into Pipeline\n",
    "\n",
    "Let's convert dirty machine learning code into clean code using a [Pipeline](https://stackoverflow.com/a/60303302/2476920) - which is the [Pipe and Filter Design Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/pipes-and-filters) for Machine Learning. \n",
    "\n",
    "At first you may still wonder *why* using this Design Patterns is good. You'll realize just how good it is in the 2nd [Clean Machine Learning Kata](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code) when you'll do AutoML. Pipelines will give you the ability to easily manage the hyperparameters and the hyperparameter space, on a per-step basis. You'll also have the good code structure for training, saving, reloading, and deploying using any library you want without hitting a wall when it'll come to serializing your whole trained pipeline for deploying in prod.\n",
    "\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "It'll be downloaded automatically for you in the code below. \n",
    "\n",
    "We're using a Human Activity Recognition (HAR) dataset captured using smartphones. The [dataset](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) can be found on the UCI Machine Learning Repository. \n",
    "\n",
    "### The task\n",
    "\n",
    "Classify the type of movement amongst six categories from the phones' sensor data:\n",
    "- WALKING,\n",
    "- WALKING_UPSTAIRS,\n",
    "- WALKING_DOWNSTAIRS,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING.\n",
    "\n",
    "### Video dataset overview\n",
    "\n",
    "Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"Video of the experiment\" width=\"400\" height=\"300\" border=\"10\" /></a>\n",
    "  <a href=\"https://youtu.be/XOEN9W05_4A\"><center>[Watch video]</center></a>\n",
    "</p>\n",
    "\n",
    "### Details about the input data\n",
    "\n",
    "The dataset's description goes like this:\n",
    "\n",
    "> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. \n",
    "\n",
    "Reference: \n",
    "> Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n",
    "\n",
    "That said, I will use the almost raw data: only the gravity effect has been filtered out of the accelerometer  as a preprocessing step for another 3D feature as an input to help learning. If you'd ever want to extract the gravity by yourself, you could use the following [Butterworth Low-Pass Filter (LPF)](https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform) and edit it to have the right cutoff frequency of 0.3 Hz which is a good frequency for activity recognition from body sensors.\n",
    "\n",
    "Here is how the 3D data cube looks like. So we'll have a train and a test data cube, and might create validation data cubes as well: \n",
    "\n",
    "![](time-series-data.jpg)\n",
    "\n",
    "So we have 3D data of shape `[batch_size, time_steps, features]`. If this and the above is still unclear to you, you may want to [learn more on the 3D shape of time series data](https://www.quora.com/What-do-samples-features-time-steps-mean-in-LSTM/answer/Guillaume-Chevalier-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code\n",
      " data\t\t\t\t\t\t        README.md\n",
      " data_loading.py\t\t\t\t        requirements.txt\n",
      "'Kata 1 - Refactor Dirty ML Code into Pipeline.ipynb'   time-series-data.jpg\n",
      " other.py\t\t\t\t\t        time-series-data.xcf\n",
      " __pycache__\t\t\t\t\t        venv\n",
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code/data\n",
      " download_dataset.py   source.txt\t 'UCI HAR Dataset.zip'\n",
      " __MACOSX\t      'UCI HAR Dataset'\n",
      "\n",
      "Downloading...\n",
      "Dataset already downloaded. Did not download twice.\n",
      "\n",
      "Extracting...\n",
      "Dataset already extracted. Did not extract twice.\n",
      "\n",
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code/data\n",
      " download_dataset.py   source.txt\t 'UCI HAR Dataset.zip'\n",
      " __MACOSX\t      'UCI HAR Dataset'\n",
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code\n",
      " data\t\t\t\t\t\t        README.md\n",
      " data_loading.py\t\t\t\t        requirements.txt\n",
      "'Kata 1 - Refactor Dirty ML Code into Pipeline.ipynb'   time-series-data.jpg\n",
      " other.py\t\t\t\t\t        time-series-data.xcf\n",
      " __pycache__\t\t\t\t\t        venv\n",
      "\n",
      "Dataset is now located at: data/UCI HAR Dataset/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "# Note: Linux bash commands start with a \"!\" inside those \"ipython notebook\" cells\n",
    "!pwd && ls\n",
    "os.chdir(DATA_PATH)\n",
    "!pwd && ls\n",
    "!python download_dataset.py\n",
    "!pwd && ls\n",
    "os.chdir(\"..\")\n",
    "!pwd && ls\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(2947, 128, 9) (2947, 1) 0.09913992 0.39567086\n"
     ]
    }
   ],
   "source": [
    "from data_loading import load_all_data\n",
    "X_train, y_train, X_test, y_test = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Define Pipeline Steps and a Pipeline\n",
    "\n",
    "The kata is to fill the classes below and to use them properly in the pipeline thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.base import BaseStep, NonFittableMixin\n",
    "from neuraxle.steps.numpy import NumpyConcatenateInnerFeatures, NumpyShapePrinter, NumpyFlattenDatum\n",
    "\n",
    "class NumpyFFT(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Featurize time series data with FFT.\n",
    "\n",
    "        :param data_inputs: time series data of 3D shape: [batch_size, time_steps, sensors_readings]\n",
    "        :return: featurized data is of 2D shape: [batch_size, n_features]\n",
    "        \"\"\"\n",
    "        transformed_data = np.fft.rfft(data_inputs, axis=-2)\n",
    "        return transformed_data\n",
    "\n",
    "\n",
    "class FFTPeakBinWithValue(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will compute peak fft bins (int), and their magnitudes' value (float), to concatenate them.\n",
    "\n",
    "        :param data_inputs: real magnitudes of an fft. It could be of shape [batch_size, bins, features].\n",
    "        :return: Two arrays without bins concatenated on feature axis. Shape: [batch_size, 2 * features]\n",
    "        \"\"\"\n",
    "        time_bins_axis = -2\n",
    "        peak_bin = np.argmax(data_inputs, axis=time_bins_axis)\n",
    "        peak_bin_val = np.max(data_inputs, axis=time_bins_axis)\n",
    "        \n",
    "        # Notice that here another FeatureUnion could have been used with a joiner:\n",
    "        transformed = np.concatenate([peak_bin, peak_bin_val], axis=-1)\n",
    "        \n",
    "        return transformed\n",
    "\n",
    "\n",
    "class NumpyAbs(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a max.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.abs(data_inputs)\n",
    "\n",
    "\n",
    "class NumpyMean(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a mean.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.mean(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMedian(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a median.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.median(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMin(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a min.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.min(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMax(NonFittableMixin, BaseStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a max.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.max(data_inputs, axis=-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the Pipeline with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gui/Documents/GIT/Kata-Clean-Machine-Learning-From-Dirty-Code/venv/src/neuraxle/neuraxle/base.py:2447: UserWarning: Named pipeline tuples must be unique. Will rename 'TrainOnlyWrapper' because it already exists.\n",
      "  \"Will rename '{}' because it already exists.\".format(class_name))\n"
     ]
    }
   ],
   "source": [
    "from neuraxle.base import Identity\n",
    "from neuraxle.pipeline import Pipeline\n",
    "from neuraxle.steps.flow import TrainOnlyWrapper\n",
    "from neuraxle.union import FeatureUnion\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # ToNumpy(),  # Cast type in case it was a list.\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(\n",
    "        # For debugging, do this print at train-time only:\n",
    "        custom_message=\"Input shape before feature union\"\n",
    "        # Shape: [batch_size, time_steps, sensor_features]\n",
    "    )),\n",
    "    FeatureUnion([\n",
    "        Pipeline([\n",
    "            NumpyFFT(),\n",
    "            NumpyAbs(),\n",
    "            FeatureUnion([\n",
    "                NumpyFlattenDatum(),  # Reshape from 3D to flat 2D: flattening data except on batch size\n",
    "                FFTPeakBinWithValue()  # Extract 2D features from the 3D FFT bins\n",
    "            ], joiner=NumpyConcatenateInnerFeatures())\n",
    "        ]),\n",
    "        NumpyMean(),\n",
    "        NumpyMedian(),\n",
    "        NumpyMin(),\n",
    "        NumpyMax()\n",
    "    ], joiner=NumpyConcatenateInnerFeatures()),  # The joiner will here join like this: np.concatenate([...], axis=-1)\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(\n",
    "        custom_message=\"Shape after feature union, before classification\"\n",
    "        # Shape: [batch_size, remade_features]\n",
    "    )),\n",
    "    # TODO in kata 2: Add some feature selection right here for the motivated ones:\n",
    "    #      https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "    # TODO in kata 2: Add normalization right here (if using other classifiers)\n",
    "    #      https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "    DecisionTreeClassifier(),\n",
    "    # TODO in kata 2: Try other classifiers different than the DecisionTreeClassifier just above:\n",
    "    #      https://scikit-learn.org/stable/modules/multiclass.html\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(\n",
    "        custom_message=\"Shape at output after classification\"\n",
    "        # Shape: [batch_size]\n",
    "    )), Identity()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Code: Make the Tests Pass\n",
    "\n",
    "The 3rd test is the real deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test '_test_is_pipeline(pipeline)' succeed!\n",
      "==> Test '_test_has_all_data_preprocessors(pipeline)' succeed!\n",
      "NumpyShapePrinter: (7352, 128, 9) Input shape before feature union\n",
      "NumpyShapePrinter: (7352, 639) Shape after feature union, before classification\n",
      "NumpyShapePrinter: (7352,) Shape at output after classification\n",
      "Test accuracy score: 0.8652867322701052\n",
      "==> Test '_test_pipeline_words_and_has_ok_score(pipeline)' succeed!\n"
     ]
    }
   ],
   "source": [
    "def _test_is_pipeline(pipeline):\n",
    "    assert isinstance(pipeline, Pipeline)\n",
    "\n",
    "\n",
    "def _test_has_all_data_preprocessors(pipeline):\n",
    "    assert \"DecisionTreeClassifier\" in pipeline\n",
    "    assert \"FeatureUnion\" in pipeline\n",
    "    assert \"Pipeline\" in pipeline[\"FeatureUnion\"]\n",
    "    assert \"NumpyMean\" in pipeline[\"FeatureUnion\"]\n",
    "    assert \"NumpyMedian\" in pipeline[\"FeatureUnion\"]\n",
    "    assert \"NumpyMin\" in pipeline[\"FeatureUnion\"]\n",
    "    assert \"NumpyMax\" in pipeline[\"FeatureUnion\"]\n",
    "\n",
    "\n",
    "def _test_pipeline_words_and_has_ok_score(pipeline):\n",
    "    pipeline = pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test accuracy score:\", accuracy)\n",
    "    assert accuracy > 0.7\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tests = [_test_is_pipeline, _test_has_all_data_preprocessors, _test_pipeline_words_and_has_ok_score]\n",
    "    for t in tests:\n",
    "        try:\n",
    "            t(pipeline)\n",
    "            print(\"==> Test '{}(pipeline)' succeed!\".format(t.__name__))\n",
    "        except Exception as e:\n",
    "            print(\"==> Test '{}(pipeline)' failed:\".format(t.__name__))\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good job!\n",
    "\n",
    "Your code should now be clean after making the tests pass.\n",
    "\n",
    "## You're ready for [part 2](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code).\n",
    "\n",
    "You should now be ready for the 2nd [Clean Machine Learning Kata](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code). Note that the solutions are available in the repository above as well. \n",
    "\n",
    "## Recommended additional readings and learning resources: \n",
    "\n",
    "- For more info on clean machine learning, you may want to read [How to Code Neat Machine Learning Pipelines](https://www.neuraxio.com/en/blog/neuraxle/2019/10/26/neat-machine-learning-pipelines.html).\n",
    "- For reaching higher performances, you could use a [LSTM Recurrent Neural Network](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) and refactoring it into a neat pipeline as you've created here, now by [using TensorFlow in your ML pipeline](https://github.com/Neuraxio/Neuraxle-TensorFlow).\n",
    "- You may as well want to request [more training and coaching for your ML or time series processing projects](https://www.neuraxio.com/en/time-series-solution) from us if you need.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "clean-ml-kata",
   "language": "python",
   "name": "clean-ml-kata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
